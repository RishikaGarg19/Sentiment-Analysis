{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from time import time\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:56:54.021152Z","iopub.execute_input":"2022-10-07T02:56:54.021514Z","iopub.status.idle":"2022-10-07T02:56:54.026041Z","shell.execute_reply.started":"2022-10-07T02:56:54.021470Z","shell.execute_reply":"2022-10-07T02:56:54.025049Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df_raw = pd.read_csv('../input/sentiment140/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\", header=None)\n\ndf_raw.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n\ndf_raw.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:56:54.032995Z","iopub.execute_input":"2022-10-07T02:56:54.033299Z","iopub.status.idle":"2022-10-07T02:56:59.810353Z","shell.execute_reply.started":"2022-10-07T02:56:54.033239Z","shell.execute_reply":"2022-10-07T02:56:59.809420Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   label        time                          date     query         username  \\\n0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n\n                                                text  \n0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n1  is upset that he can't update his Facebook by ...  \n2  @Kenichan I dived many times for the ball. Man...  \n3    my whole body feels itchy and like its on fire   \n4  @nationwideclass no, it's not behaving at all....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>time</th>\n      <th>date</th>\n      <th>query</th>\n      <th>username</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_raw['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:56:59.812113Z","iopub.execute_input":"2022-10-07T02:56:59.812472Z","iopub.status.idle":"2022-10-07T02:56:59.834424Z","shell.execute_reply.started":"2022-10-07T02:56:59.812421Z","shell.execute_reply":"2022-10-07T02:56:59.833619Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"4    800000\n0    800000\nName: label, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df = df_raw[['label', 'text']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:56:59.836117Z","iopub.execute_input":"2022-10-07T02:56:59.836471Z","iopub.status.idle":"2022-10-07T02:56:59.884580Z","shell.execute_reply.started":"2022-10-07T02:56:59.836418Z","shell.execute_reply":"2022-10-07T02:56:59.883519Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   label                                               text\n0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n1      0  is upset that he can't update his Facebook by ...\n2      0  @Kenichan I dived many times for the ball. Man...\n3      0    my whole body feels itchy and like its on fire \n4      0  @nationwideclass no, it's not behaving at all....","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"start_time = time()\n\nfrom nltk.tokenize import TweetTokenizer\ntk = TweetTokenizer(reduce_len=True)\n\ndata = []\n\nX = df['text'].tolist()\nY = df['label'].tolist()\n\nfor x, y in zip(X, Y):\n    if y == 4:\n        data.append((tk.tokenize(x), 1))\n    else:\n        data.append((tk.tokenize(x), 0))\n        \nprint('CPU Time:', time() - start_time)\ndata[:5]","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:56:59.887699Z","iopub.execute_input":"2022-10-07T02:56:59.887942Z","iopub.status.idle":"2022-10-07T02:58:14.391058Z","shell.execute_reply.started":"2022-10-07T02:56:59.887917Z","shell.execute_reply":"2022-10-07T02:58:14.390123Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"CPU Time: 74.48767423629761\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[(['@switchfoot',\n   'http://twitpic.com/2y1zl',\n   '-',\n   'Awww',\n   ',',\n   \"that's\",\n   'a',\n   'bummer',\n   '.',\n   'You',\n   'shoulda',\n   'got',\n   'David',\n   'Carr',\n   'of',\n   'Third',\n   'Day',\n   'to',\n   'do',\n   'it',\n   '.',\n   ';D'],\n  0),\n (['is',\n   'upset',\n   'that',\n   'he',\n   \"can't\",\n   'update',\n   'his',\n   'Facebook',\n   'by',\n   'texting',\n   'it',\n   '...',\n   'and',\n   'might',\n   'cry',\n   'as',\n   'a',\n   'result',\n   'School',\n   'today',\n   'also',\n   '.',\n   'Blah',\n   '!'],\n  0),\n (['@Kenichan',\n   'I',\n   'dived',\n   'many',\n   'times',\n   'for',\n   'the',\n   'ball',\n   '.',\n   'Managed',\n   'to',\n   'save',\n   '50',\n   '%',\n   'The',\n   'rest',\n   'go',\n   'out',\n   'of',\n   'bounds'],\n  0),\n (['my',\n   'whole',\n   'body',\n   'feels',\n   'itchy',\n   'and',\n   'like',\n   'its',\n   'on',\n   'fire'],\n  0),\n (['@nationwideclass',\n   'no',\n   ',',\n   \"it's\",\n   'not',\n   'behaving',\n   'at',\n   'all',\n   '.',\n   \"i'm\",\n   'mad',\n   '.',\n   'why',\n   'am',\n   'i',\n   'here',\n   '?',\n   'because',\n   'I',\n   \"can't\",\n   'see',\n   'you',\n   'all',\n   'over',\n   'there',\n   '.'],\n  0)]"},"metadata":{}}]},{"cell_type":"code","source":"from nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nprint(pos_tag(data[0][0]))","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:58:14.394505Z","iopub.execute_input":"2022-10-07T02:58:14.394774Z","iopub.status.idle":"2022-10-07T02:58:14.500547Z","shell.execute_reply.started":"2022-10-07T02:58:14.394745Z","shell.execute_reply":"2022-10-07T02:58:14.499627Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[('@switchfoot', 'NN'), ('http://twitpic.com/2y1zl', 'SYM'), ('-', ':'), ('Awww', 'NNP'), (',', ','), (\"that's\", 'VBZ'), ('a', 'DT'), ('bummer', 'NN'), ('.', '.'), ('You', 'PRP'), ('shoulda', 'VBP'), ('got', 'VBD'), ('David', 'NNP'), ('Carr', 'NNP'), ('of', 'IN'), ('Third', 'NNP'), ('Day', 'NNP'), ('to', 'TO'), ('do', 'VB'), ('it', 'PRP'), ('.', '.'), (';D', 'VB')]\n","output_type":"stream"}]},{"cell_type":"code","source":"def lemmatize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\n\nprint(lemmatize_sentence(data[0][0]))","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:58:14.503151Z","iopub.execute_input":"2022-10-07T02:58:14.503679Z","iopub.status.idle":"2022-10-07T02:58:16.729738Z","shell.execute_reply.started":"2022-10-07T02:58:14.503638Z","shell.execute_reply":"2022-10-07T02:58:16.728829Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['@switchfoot', 'http://twitpic.com/2y1zl', '-', 'Awww', ',', \"that's\", 'a', 'bummer', '.', 'You', 'shoulda', 'get', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';D']\n","output_type":"stream"}]},{"cell_type":"code","source":"import re, string\nfrom nltk.corpus import stopwords\nSTOP_WORDS = stopwords.words('english')\n\ndef cleaned(token):\n    if token == 'u':\n        return 'you'\n    if token == 'r':\n        return 'are'\n    if token == 'some1':\n        return 'someone'\n    if token == 'yrs':\n        return 'years'\n    if token == 'hrs':\n        return 'hours'\n    if token == 'mins':\n        return 'minutes'\n    if token == 'secs':\n        return 'seconds'\n    if token == 'pls' or token == 'plz':\n        return 'please'\n    if token == '2morow':\n        return 'tomorrow'\n    if token == '2day':\n        return 'today'\n    if token == '4got' or token == '4gotten':\n        return 'forget'\n    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == 'Â½25':\n        return ''\n    return token\n\ndef remove_noise(tweet_tokens):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(tweet_tokens):\n        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        cleaned_token = cleaned(token.lower())\n        \n        if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOP_WORDS:\n            cleaned_tokens.append(cleaned_token)\n            \n    return cleaned_tokens\n\nprint(remove_noise(data[0][0]))","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:58:16.731037Z","iopub.execute_input":"2022-10-07T02:58:16.731561Z","iopub.status.idle":"2022-10-07T02:58:16.750072Z","shell.execute_reply.started":"2022-10-07T02:58:16.731510Z","shell.execute_reply":"2022-10-07T02:58:16.749144Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['awww', \"that's\", 'bummer', 'shoulda', 'get', 'david', 'carr', 'third', 'day']\n","output_type":"stream"}]},{"cell_type":"code","source":"start_time = time()\n\ndef list_to_dict(cleaned_tokens):\n    return dict([token, True] for token in cleaned_tokens)\n\ncleaned_tokens_list = []\n\nfor tokens, label in data:\n    cleaned_tokens_list.append((remove_noise(tokens), label))\n\nprint('Removed Noise, CPU Time:', time() - start_time)\nstart_time = time()\n\nfinal_data = []\n\nfor tokens, label in cleaned_tokens_list:\n    final_data.append((list_to_dict(tokens), label))\n    \nprint('Data Prepared for model, CPU Time:', time() - start_time)\n\nfinal_data[:5]","metadata":{"execution":{"iopub.status.busy":"2022-10-07T02:58:16.751803Z","iopub.execute_input":"2022-10-07T02:58:16.752385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.Random(140).shuffle(final_data)\ntrim_index = int(len(final_data) * 0.9)\n\ntrain_data = final_data[:trim_index]\ntest_data = final_data[trim_index:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_glove_vecs(glove_file):\n    with open(glove_file, 'r', encoding=\"utf8\") as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time()\n\nunks = []\nUNKS = []\n\ndef cleared(word):\n    res = \"\"\n    prev = None\n    for char in word:\n        if char == prev: continue\n        prev = char\n        res += char\n    return res\n\n\ndef sentence_to_indices(sentence_words, word_to_index, max_len, i):\n    global X, Y\n    sentence_indices = []\n    for j, w in enumerate(sentence_words):\n        try:\n            index = word_to_index[w]\n        except:\n            UNKS.append(w)\n            w = cleared(w)\n            try:\n                index = word_to_index[w]\n            except:\n                index = word_to_index['unk']\n                unks.append(w)\n        X[i, j] = index\n\nprint('Removed Noise, CPU Time:', time() - start_time)\nstart_time = time()\n\nlist_len = [len(i) for i, j in cleaned_tokens_list]\nmax_len = max(list_len)\nprint('max_len:', max_len)\n\nX = np.zeros((len(cleaned_tokens_list), max_len))\nY = np.zeros((len(cleaned_tokens_list), ))\n\nfor i, tk_lb in enumerate(cleaned_tokens_list):\n    tokens, label = tk_lb\n    sentence_to_indices(tokens, word_to_index, max_len, i)\n    Y[i] = label\n    \nprint('Data Prepared for model, CPU Time:', time() - start_time)\n\n\nprint(X[:5])\nprint(Y[:5])","metadata":{"id":"plIFObh4jYJV","outputId":"cc21833e-e053-454b-d410-b8e9d48b4392","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len):\n    vocab_len = len(word_to_index) + 1\n    emb_dim = word_to_vec_map[\"unk\"].shape[0] #50\n    \n    emb_matrix = np.zeros((vocab_len, emb_dim))\n    \n    for word, idx in word_to_index.items():\n        emb_matrix[idx, :] = word_to_vec_map[word]\n        \n    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False, input_shape=(max_len,))\n    embedding_layer.build((None,))\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","metadata":{"id":"_ugE7pGBm8A-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=False)))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\nmodel.summary()","metadata":{"id":"UZvgewT9Ev4c","outputId":"53cb7607-e27e-4c23-a5fd-e31ec09d68bb","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"id":"TFHoaATvEv4f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)","metadata":{"id":"aUvi0G_mEv4g","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 20, batch_size = 128, shuffle=True)","metadata":{"id":"R75KvAX1Ev4m","outputId":"6db703ff-9281-4e52-b5e1-adb63ffb281b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_acc_loss(history):\n    \n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n\n    plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_acc_loss(model.history)","metadata":{"id":"t2RSp_d8Ev4o","outputId":"7e7efca4-73d7-48a3-9ec9-f28b4addda8c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.6. Predicting on Custom Data <a class=\"anchor\" id=\"head-5-6\"></a>","metadata":{}},{"cell_type":"code","source":"def sentence_to_indices(sentence_words, max_len):\n    X = np.zeros((max_len))\n    sentence_indices = []\n    for j, w in enumerate(sentence_words):\n        try:\n            index = word_to_index[w]\n        except:\n            w = cleared(w)\n            try:\n                index = word_to_index[w]\n            except:\n                index = word_to_index['unk']\n        X[j] = index\n    return X\n\ndef predict_custom_tweet_sentiment(custom_tweet):\n    x_input = sentence_to_indices(remove_noise(tk.tokenize(custom_tweet)), max_len)\n    \n    return model.predict(np.array([x_input])).item()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_custom_tweet_sentiment(\"I'm happy you're here!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_custom_tweet_sentiment(\"I'm not happy you're here!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_custom_tweet_sentiment(\"I disliked his attitude...\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_custom_tweet_sentiment(\"I'm feeling sentimental\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.7. Inspecting Wrongly Predicted Data <a class=\"anchor\" id=\"head-5-7\"></a>","metadata":{}},{"cell_type":"code","source":"def i_to_sentence(I):\n    sentence = \"\"\n    for i in I:\n        if i:\n            sentence += index_to_word[int(i)] + \" \"\n        else:\n            break\n    return sentence","metadata":{"id":"a4D7HfxUQ4K2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"C = 0\n\npred = model_clean_data.predict(X_test)\n\nfor i in range(len(X_test)):\n    final_pred = 1 if pred[i] > 0.5 else 0\n    \n    if(final_pred != Y_test[i]):\n        print('Expected sentiment: ' + str(int(Y_test[i])) + '. Input: ' + i_to_sentence(X_test[i]))\n        C += 1\n        \n    if C > 100:\n        break","metadata":{"id":"3brr15ME7NVz","outputId":"24af0c31-bb98-4af0-fe6f-e7c7acf26f6a","trusted":true},"execution_count":null,"outputs":[]}]}